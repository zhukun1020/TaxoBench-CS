#  Prompt Sensitivity and Robustness in LLMs
 {"Papers": [0, 2, 7, 24, 29, 37, 56, 65, 68, 78, 79, 83, 103, 104, 105]}
##  LLMs for Contextual Robustness and Sensible Responses
 {"Papers": [0, 79]}
##  Prompt Sensitivity and Trustworthiness in LLMs
 {"Papers": [2, 7, 29, 65, 68, 104]}
##  Instruction Variability and Robustness in Task Generalization
 {"Papers": [24, 56]}
##  Advanced Prompting Techniques for Enhanced Reasoning
 {"Papers": [37, 78, 83, 103, 105]}
#  Uncertainty Quantification and Knowledge Boundaries in LLMs
 {"Papers": [1, 6, 12, 13, 18, 21, 26, 31, 32, 34, 36, 39, 44, 46, 48, 49, 52, 53, 58, 60, 61, 62, 80, 85, 87, 88, 89, 90, 93, 94, 95, 99, 106, 108]}
##  LLMs' Intrinsic Knowledge Boundary Assessment
 {"Papers": [1, 6, 12, 34, 36, 49, 60, 61, 80, 85, 89, 95, 99, 108]}
###  Self-Knowledge and Uncertainty Articulation in LLMs
 {"Papers": [1, 60, 80, 95, 108]}
###  Internal Signal Utilization for Hallucination Detection
 {"Papers": [6, 12, 34, 36, 49, 99]}
###  Benchmarking Comprehension and Reasoning in LLMs
 {"Papers": [61]}
###  Abstention Strategies for Mitigating Hallucinations
 {"Papers": [85, 89]}
##  Collaborative Techniques for LLMs' Knowledge Gap Identification
 {"Papers": [13, 21, 32, 48, 52, 58, 88, 90]}
##  Semantic-Based Uncertainty Quantification in LLMs
 {"Papers": [18, 26, 39, 44, 46, 53, 62, 87, 93, 106]}
###  Token-Level Relevance for Hallucination Detection
 {"Papers": [18, 93]}
###  Cascading Strategies for Cost-Effective Uncertainty Management
 {"Papers": [26, 62]}
###  Semantic Entropy and Verbalized Confidence in Uncertainty Quantification
 {"Papers": [39, 44, 46, 87, 106]}
###  Error Detection in Autoregressive Structured Prediction
 {"Papers": [53]}
##  Self-aware Retrieval Augmented Generation
 {"Papers": [31, 94]}
#  Hallucination Mitigation and Truthfulness in LLMs
 {"Papers": [3, 4, 10, 11, 14, 15, 17, 19, 20, 23, 30, 33, 35, 38, 41, 43, 45, 47, 50, 54, 55, 63, 66, 67, 73, 74, 76, 77, 84, 86, 91, 92, 97, 98, 100]}
##  Internal State Analysis for Truthfulness Detection
 {"Papers": [3, 14, 19, 43, 47, 50, 55]}
##  Decoding Strategies for Hallucination Mitigation
 {"Papers": [4, 10, 15, 17, 41, 45, 54, 67, 97, 98]}
###  Multimodal Verification and Contrastive Decoding for Hallucination Reduction
 {"Papers": [4, 17, 41, 54, 97, 98]}
###  Contextual Sharpness and Context-aware Decoding for Enhanced Factuality
 {"Papers": [10, 67]}
###  Layer Contrastive Decoding for Improved Model Truthfulness
 {"Papers": [15]}
###  Factuality-Aware Fine-Tuning for Knowledge Alignment
 {"Papers": [45]}
##  Uncertainty Quantification for Hallucination Detection
 {"Papers": [11, 20, 23, 30, 33, 35, 38, 63, 73, 74, 76, 77, 86, 91, 92, 100]}
###  Internal State Analysis for Hallucination Detection
 {"Papers": [11, 30, 33, 38, 74, 76, 91, 100]}
###  Token-Level Uncertainty Quantification for Fact-Checking
 {"Papers": [20, 63, 73]}
###  Self-Reflective Metrics for Autonomous Honesty
 {"Papers": [23, 92]}
###  Fine-Tuning Strategies for Hallucination Control
 {"Papers": [35, 77, 86]}
##  Sycophancy Reduction through Synthetic Data Intervention
 {"Papers": [66, 84]}
#  Confidence Calibration and Self-Assessment in LLMs
 {"Papers": [5, 8, 9, 16, 22, 25, 27, 28, 40, 42, 51, 57, 59, 64, 69, 70, 71, 72, 75, 81, 82, 96, 101, 102, 107]}
##  Confidence Calibration Techniques for Long-Form LLM Generations
 {"Papers": [5, 8, 22, 27, 28, 51, 64, 69, 70, 72, 75, 102, 107]}
###  Linguistic and Distributional Confidence Calibration Techniques
 {"Papers": [5, 22, 28, 51, 70, 72, 75, 102, 107]}
####  Linguistic Calibration for Long-Form Generations
 {"Papers": [5, 28, 51]}
####  Confidence Estimation for Hallucination Mitigation
 {"Papers": [22, 70, 72, 107]}
####  Generation-Based Confidence Prediction
 {"Papers": [75]}
####  Fact-and-Reflection Prompting for Confidence Calibration
 {"Papers": [102]}
###  Self-Evaluation and Experience-Based Confidence Calibration
 {"Papers": [8, 27]}
###  Selective Generation with Self-Evaluation for Confidence Expression
 {"Papers": [64]}
###  Inter-Expert Agreement for Confidence Calibration in Mixture Models
 {"Papers": [69]}
##  Self-Consistency Strategies for Enhanced LLM Reasoning
 {"Papers": [9, 42, 71, 81, 82, 96]}
##  Calibration Methods for Accurate Probability Estimation in Neural Networks
 {"Papers": [16, 25, 40, 57, 59]}
##  Knowledge-Enhanced Verification for Hallucination Detection in LLMs
 {"Papers": [101]}